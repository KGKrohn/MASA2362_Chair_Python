import xgboost as xgb
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

from utils import *

# Made with good help from ChatGPT

# Plot the validation dataset and mark predictions (Generated by ChatGPT)
def plot_predictions(data, predictions, title):
    plt.figure(figsize=(12, 6))

    # Plot all features except the target column
    for col in data.columns[:-1]:  # Exclude the target column
        plt.plot(data.index, data[col], alpha=0.6, label=f"Sensor: {col}")

    # Shade areas where target == 1
    target_col = data.iloc[:, -1]
    in_sitting_period = False
    for i in range(len(target_col)):
        if target_col.iloc[i] == 1 and not in_sitting_period:
            # Start of sitting period
            start = i
            in_sitting_period = True
        elif target_col.iloc[i] == 0 and in_sitting_period:
            # End of sitting period
            end = i
            plt.axvspan(start, end, color='blue', alpha=0.2, label="Sitting Period" if start == 0 else None)
            in_sitting_period = False
    if in_sitting_period:
        # Shade till the end if still sitting
        plt.axvspan(start, len(target_col), color='blue', alpha=0.2, label="Sitting Period")

    # Highlight predictions where the model predicts sitting (1)
    sitting_indices = data.index[predictions == 1]
    plt.scatter(sitting_indices, [data.iloc[i, :-1].mean() for i in sitting_indices],
                color='red', label="Predicted Sitting", zorder=5)

    plt.title(title)
    plt.xlabel("Sample Index")
    plt.ylabel("Sensor Values")
    plt.legend(loc="upper right", fontsize="small")
    plt.grid(True)
    plt.show()

# Load the datasets
training_data = load_csv("Old_data/sit_down_2.csv")
validation_data = load_csv("Old_data/sit_down_4.csv")

# Specify columns to drop (Due to sensors being wack)
columns_to_drop = [1,2,9,10]

# Drop specified columns
training_data = training_data.drop(columns=columns_to_drop, errors='ignore')
validation_data = validation_data.drop(columns=columns_to_drop, errors='ignore')

# Apply a rolling average to the features
def apply_rolling_average(df, window_size=5):
    # Apply rolling average to all features except the target column
    rolling_features = df.iloc[:, :-1].rolling(window=window_size, min_periods=1).mean()
    rolling_features["target"] = df.iloc[:, -1]  # Reattach the target column
    return rolling_features

training_data = apply_rolling_average(training_data, window_size=3)
validation_data = apply_rolling_average(validation_data, window_size=3)

# Specify the target column position
target_column_position = -1

# Select features and target
X_train = training_data.iloc[:, :target_column_position]  # Features
y_train = training_data.iloc[:, target_column_position]   # Target

X_validation = validation_data.iloc[:, :target_column_position]
y_validation = validation_data.iloc[:, target_column_position]

# Verify the target is binary
assert set(y_train.unique()) == {0, 1}, "Target column in training data is not binary."
assert set(y_validation.unique()) == {0, 1}, "Target column in validation data is not binary."

# Train XGBoost model
xgb_model = xgb.XGBClassifier(
    n_estimators=300,
    learning_rate=0.01,
    max_depth=10,
    scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]),
    random_state=42,
    eval_metric="logloss"
)

# Train the model
xgb_model.fit(X_train, y_train)

# Predict on the validation set
y_pred_rf = xgb_model.predict(X_validation)

# Evaluate the model
accuracy_rf = accuracy_score(y_validation, y_pred_rf)
report_rf = classification_report(y_validation, y_pred_rf)

print(f"Accuracy: {accuracy_rf:.4f}")
print(report_rf)

# Plot validation dataset with predictions
plot_predictions(validation_data, y_pred_rf, "Validation Data with Predicted Sitting Points")
